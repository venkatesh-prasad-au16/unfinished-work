# -*- coding: utf-8 -*-
"""FI_Model_Development_(Keras)_v5 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aF2lzxuyZaJB7tLnXJd1dCcDfYoHA-A9

Keras CNN Model with Relu Activation and 0.6 Drop out
"""



from __future__ import print_function, division
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import copy
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tqdm import tqdm_notebook as tqdm
from sklearn.preprocessing import LabelEncoder
import PIL
from PIL import Image
import random
from scipy import ndarray

# image processing library
import skimage as sk
from skimage import transform
from skimage import util
from skimage import io
from skimage.util import img_as_ubyte, img_as_float
from skimage.transform import warp, AffineTransform, ProjectiveTransform
from skimage.exposure import equalize_adapthist, equalize_hist, rescale_intensity, adjust_gamma, adjust_log, adjust_sigmoid
from skimage.filters import gaussian
from skimage.util import random_noise



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import h5py

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   print(
#       '\n\nThis error most likely means that this notebook is not '
#       'configured to use a GPU.  Change this in Notebook Settings via the '
#       'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
#   raise SystemError('GPU device not found')

# Load the Drive helper and mount
# from google.colab import drive
# drive.mount('/content/drive/')
#drive.mount('/content/drive/', force_remount=True) #DC updated
FI_dir = "/home/venkatesh/Desktop/Curafoot/Foots"

#print(os.listdir(FI_dir))

image=[]
labels=[]
for f in os.listdir(FI_dir):
  if f=='MILD':
    for c in os.listdir(os.path.join(FI_dir, f)):
      image.append(os.path.join(FI_dir, f,c))
      labels.append('MILD')
  if f=='SEVERE':
    for c in os.listdir(os.path.join(FI_dir, f)):
      image.append(os.path.join(FI_dir, f,c))
      labels.append('SEVERE')
  if f=='MODERATE':
    for c in os.listdir(os.path.join(FI_dir, f)):
      image.append(os.path.join(FI_dir, f,c))
      labels.append('MODERATE')
  if f=='HIGH ARCH':
      for c in os.listdir(os.path.join(FI_dir, f)):
        image.append(os.path.join(FI_dir, f,c))
        labels.append('HIGH ARCH')
  if f=='NORMAL ARCH':
      for c in os.listdir(os.path.join(FI_dir, f)):
        image.append(os.path.join(FI_dir, f,c))
        labels.append('NORMAL ARCH')
imagedata = {'Images':image, 'labels':labels} 
image_data = pd.DataFrame(imagedata)

#image_data.head()
image_data.groupby(['labels']).size()

image_size = (500, 500)
batch_size = 1

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    FI_dir,
    validation_split=0.2,
    subset="training",
    seed=3333,
    image_size=image_size,
    batch_size=batch_size,
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    FI_dir,
    validation_split=0.2,
    subset="validation",
    seed=3333,
    image_size=image_size,
    batch_size=batch_size,
)

train_ds

from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
#from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

model = Sequential()
model.add(Conv2D(32,3,padding="same", activation="relu", input_shape=(500,500,3)))
model.add(MaxPool2D())
model.add(Dropout(0.6))
model.add(Flatten())
model.add(Dense(5, activation="softmax"))

model.summary()

# opt = Adam(lr=0.000001)
opt = Adam(lr=0.0001) #DC updated
# model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])
model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) , metrics = ['accuracy']) #DC updated

history = model.fit(train_ds, epochs = 5 , validation_data = val_ds)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(5)

plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#from keras.models import model_from_json

# serialize model to JSON
#model_json = model.to_json()

#with open("/content/drive/MyDrive/Foot images3/model_v3.json", "w") as json_file:
#    json_file.write(model_json)
#model.save_weights("/content/drive/MyDrive/Foot images3/model_v3.h5")
#print("Saved model to disk")